{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook performs predictive modeling on the Heart Diseases UCI (https://www.kaggle.com/ronitf/heart-disease-uci) to identify relationship between heart disease and various other features. \n",
    "\n",
    "## Step1:  Select and load python libraries\n",
    "Python libraries loaded to preform the prelimnary EDA include Pandas, Numpy, sklearn, Matplotlb, and Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Load Conditioned Dataset from Exploratory Data Analysis\n",
    "The dataset that was conditioned as part of the Exploratory Data Analysis portion of this project, and saved as a pickel file for use in this phase of the project.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "locData=r'C:\\Users\\Lisa\\Documents\\Training\\CoderGirl\\Project\\Data'\n",
    "df=pd.read_pickle(locData+'\\\\heart.pkl')\n",
    "print('Dataframe Shape:  {}'.format(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:  Reorder data set\n",
    "The dataset appears to be orgainized in such a way that the patients that have heart disease are listed first, followed by the patients that do not have heart disease.  The dataframe was shuffled such that the heart disease target column was randomly ordered, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReOrdered=df.sample(frac=1).reset_index(drop=True)\n",
    "dfReOrdered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:  Seperate the predictor and response variables.\n",
    "The response variable (i.e. what we are trying to predict) is the target column of the dataframe, and the predictor variables (i.e. variables used to predict) are the other columns of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dfReOrdered.target\n",
    "print(y[:5])\n",
    "x=dfReOrdered.drop('target',axis=1)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5:  Seperate the dataset into training and test groups, and standardize the x-dataframe\n",
    "\n",
    "The training and test groups were selected using the train_test_split function in the sklearn model_selection tool box.  Note, a validation group was not selected for this analysis, as there was a limited amount of data and when GridSearchCV was implemented in Step 7 of this effort a cross-validation technique was implemented for hyperparameter tuning.\n",
    "\n",
    "The min-max scaler from the sklearn pre-processing toolbox was selected for feature standardization.  This was selected over the standard normal standardization as not all features appeared to be normally distributed when visually inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y)\n",
    "\n",
    "scaler=MinMaxScaler().fit(xtrain)\n",
    "xtrainScaled=scaler.transform(xtrain) \n",
    "xtestScaled=scaler.transform(xtest) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:  Evaluate commonly used binary classifiers\n",
    "Several commonly used binary classifiers were evaluated with default parameters to determine which predictive model may require the least amount of tuning to achieve the most accurate heart disease predictive model.\n",
    "\n",
    "The random forest classification algorithim was selected for use as a predictive model for this project, as it is commonly used and has a top accuracy score (i.e. accuracy = 0.83).  Note, the Extra Tree Classification model has the highest accuracy score (i.e. accuracy = 0.84); however, it is less widely used than the random forest algorithm and accordingly was not selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers=[['Logistic Regression :',LogisticRegression()],\n",
    "       ['Decision Tree Classification :',DecisionTreeClassifier()],\n",
    "       ['Random Forest Classification :',RandomForestClassifier()],\n",
    "       ['Gradient Boosting Classification :', GradientBoostingClassifier()],\n",
    "       ['Ada Boosting Classification :',AdaBoostClassifier()],\n",
    "       ['Extra Tree Classification :', ExtraTreesClassifier()],\n",
    "       ['K-Neighbors Classification :',KNeighborsClassifier()],\n",
    "       ['Support Vector Classification :',SVC()],\n",
    "       ['Gaussian Naive Bayes :',GaussianNB()]]\n",
    "\n",
    "cla_pred=[]\n",
    "\n",
    "for name,model in classifiers:\n",
    "    \n",
    "    model=model\n",
    "    model.fit(xtrainScaled,ytrain)\n",
    "    predictions = model.predict(xtestScaled)\n",
    "    cla_pred.append(accuracy_score(ytest,predictions))\n",
    "    print(name,accuracy_score(ytest,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7:  Tune the random forest algorithm for optimal preformance\n",
    "To identify the most influential hyper-parameters for tuning a random forest classifier, several blog post and articles were reviewed, and the most influenital hyper-parameters identified, included:  max_features, n_estimators, max_depth, min_sample_leaf, and criterion.  Accordinlgy, a grid search was developed to ideintify the optimal input value for each of the most influential hyper-parameters.  The optimal values are shown below.\n",
    "\n",
    "References:\n",
    "https://stackoverflow.com/questions/36107820/how-to-tune-parameters-in-random-forest-using-scikit-learn\n",
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "https://www.kaggle.com/hadend/tuning-random-forest-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier()\n",
    "paramGrid=[{'n_estimators':[50,100,200],'max_depth':[3,5,7,9],\n",
    "           'min_samples_leaf':[1,2,4],'criterion':['gini','entropy']}]\n",
    "gridSearch=GridSearchCV(model,paramGrid,cv=5)\n",
    "gridSearch.fit(xtrainScaled,ytrain)\n",
    "gridSearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8:  Execute the model\n",
    "Once the optimal parameter values were identified from the GridSearchCV function, a model was developed using the selected values, as shown below.  Also, accuracy, sensitiviy, and specificity were also computed model metrics.  \n",
    "\n",
    "In general, the model is over-trained as indicated by the difference of 0.12 in accuracy between the training and test model accuracies.  \n",
    "\n",
    "Sensitivity refers to the correctly identfied true positives.  For this case, approximately 80 percent were idenitfied correctly.  Specificity refers to the models ability to identify true negatives, like sensitivity, approximately 80 percent were correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(criterion='gini',max_depth=5,min_samples_leaf=2,n_estimators=100)\n",
    "model.fit(xtrainScaled,ytrain)\n",
    "yhat=model.predict(xtestScaled)\n",
    "yhat_quant=model.predict_proba(xtestScaled)[:, 1]\n",
    "yhat_bin=model.predict(xtestScaled)\n",
    "print(\"Training Accuracy :\", model.score(xtrainScaled,ytrain))\n",
    "print(\"Testing Accuracy :\", model.score(xtestScaled,ytest))\n",
    "\n",
    "cr=classification_report(ytest,yhat)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix=confusion_matrix(ytest, yhat_bin)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=sum(sum(confusion_matrix))\n",
    "\n",
    "sensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\n",
    "print('Specificity : ', specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
